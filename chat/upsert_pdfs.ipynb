{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9df66c-e3aa-42f1-9d0f-fee0ab442ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5825135e-5888-43ef-ab6a-6e9dc5ea29f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196e9b2d-d4e5-4a3f-affd-8a7dcefa2c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf97967c-193d-47fd-8bc4-9dd57dffcf93",
   "metadata": {},
   "source": [
    "### Upsert pdfs\n",
    "\n",
    "This notebook is a down and dirty way to upload a directory of pdfs to a pinecone database.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45ad90d-dabc-470e-9f6b-c7c24b3be2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\installed_software\\anaconda3\\envs\\langchain\\Lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "import openai\n",
    "import os\n",
    "\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader, PyPDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea2decf-d1b5-46a0-98ed-41a972a9770f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4accec3e-db14-442c-9591-7fbbb84fdcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is ready\n"
     ]
    }
   ],
   "source": [
    "with open('D:\\\\NLP_projects\\\\openai_api_key.txt', 'r') as file:\n",
    "    OPENAI_API_KEY = file.read().strip()\n",
    "\n",
    "\n",
    "with open('D:\\\\NLP_projects\\\\youtube_api_key.txt', 'r') as file:\n",
    "    YOUTUBE_API_KEY = file.read().strip()\n",
    "    \n",
    "with open('D:\\\\NLP_projects\\\\pinecone_api_key.txt', 'r') as file:\n",
    "    PINECONE_API_KEY = file.read().strip()\n",
    "    \n",
    "PINECONE_API_ENV = 'northamerica-northeast1-gcp'\n",
    "\n",
    "PINECONE_INDEX = 'denson-kb'\n",
    "    \n",
    "# Set OpenAI API Key    \n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\") is not None:\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    print (\"OPENAI_API_KEY is ready\")\n",
    "else:\n",
    "    print (\"OPENAI_API_KEY environment variable not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a9c82fd-f2e3-44f9-a99e-9e11a37b52dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_API_ENV)\n",
    "index = pinecone.Index(PINECONE_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b5038f7-591b-4a6b-9cac-8f949ad0d79d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 564}},\n",
       " 'total_vector_count': 564}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f55676f5-a9b9-4778-8a6e-0285f9a90823",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # If you want to delete your vectors in your index to start over, run the code below!\n",
    "# index = pinecone.Index(index_name)\n",
    "index.delete(delete_all='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82dee421-0f1a-4967-8a79-2aaaf5ebb270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e3d6d08-dbbe-4201-b63b-097edf878142",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aec95df3-59d5-4f3b-a9cd-9de50f83e1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/d/NLP_projects/langchain-pinecone-summary/chat\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dfa50cb-762e-426e-b5e6-4e964f08a02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\nlp_pdfs\\2201.11903.pdf\n",
      "..\\..\\nlp_pdfs\\2205.11916.pdf\n",
      "..\\..\\nlp_pdfs\\2206.07682.pdf\n",
      "..\\..\\nlp_pdfs\\2207.08143.pdf\n",
      "..\\..\\nlp_pdfs\\2209.00626.pdf\n",
      "..\\..\\nlp_pdfs\\2210.09261.pdf\n",
      "..\\..\\nlp_pdfs\\2210.14986.pdf\n",
      "..\\..\\nlp_pdfs\\2211.01910.pdf\n",
      "..\\..\\nlp_pdfs\\2212.03551.pdf\n",
      "..\\..\\nlp_pdfs\\2212.12633.pdf\n",
      "..\\..\\nlp_pdfs\\2302.02676.pdf\n",
      "..\\..\\nlp_pdfs\\2302.07842.pdf\n",
      "..\\..\\nlp_pdfs\\2302.09051.pdf\n",
      "..\\..\\nlp_pdfs\\2302.10329.pdf\n",
      "..\\..\\nlp_pdfs\\2302.13971.pdf\n",
      "..\\..\\nlp_pdfs\\2303.03846.pdf\n",
      "..\\..\\nlp_pdfs\\2303.07610.pdf\n",
      "..\\..\\nlp_pdfs\\2303.07992.pdf\n",
      "..\\..\\nlp_pdfs\\2303.08774.pdf\n",
      "..\\..\\nlp_pdfs\\2303.11504.pdf\n",
      "..\\..\\nlp_pdfs\\2303.11568.pdf\n",
      "..\\..\\nlp_pdfs\\2303.17491.pdf\n",
      "..\\..\\nlp_pdfs\\2303.18223.pdf\n",
      "..\\..\\nlp_pdfs\\2304.00612.pdf\n",
      "..\\..\\nlp_pdfs\\2304.01481.pdf\n",
      "..\\..\\nlp_pdfs\\2304.01665.pdf\n",
      "..\\..\\nlp_pdfs\\2304.09842.pdf\n",
      "..\\..\\nlp_pdfs\\2305.03047.pdf\n",
      "..\\..\\nlp_pdfs\\2306.00890.pdf\n",
      "..\\..\\nlp_pdfs\\Ago+03.pdf\n",
      "..\\..\\nlp_pdfs\\dong21a.pdf\n",
      "..\\..\\nlp_pdfs\\journal.pdig.0000198.pdf\n",
      "..\\..\\nlp_pdfs\\NIPS-2017-attention-is-all-you-need-Paper.pdf\n"
     ]
    }
   ],
   "source": [
    "# root = \".\"  # take the current directory as root\n",
    "\n",
    "root = '../../nlp_pdfs/'\n",
    "\n",
    "for path in Path(root).glob(\"**/*.pdf\"):\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e1ceddb-1c5b-4687-a6ca-a962c34d27ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = PyPDFDirectoryLoader(\"../../nlp_pdfs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203dee3b-13f0-4ca7-9560-60e84f047eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "553301e7-f1ee-41cb-9ba1-c729680aba44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base='', openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization='', allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=6, request_timeout=None, headers=None, tiktoken_model_name=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8de2ca36-6a57-43e0-9e26-3ab499df1322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\nlp_pdfs\\2201.11903.pdf\n",
      "..\\..\\nlp_pdfs\\2205.11916.pdf\n",
      "..\\..\\nlp_pdfs\\2206.07682.pdf\n",
      "..\\..\\nlp_pdfs\\2207.08143.pdf\n",
      "..\\..\\nlp_pdfs\\2209.00626.pdf\n",
      "..\\..\\nlp_pdfs\\2210.09261.pdf\n",
      "..\\..\\nlp_pdfs\\2210.14986.pdf\n",
      "..\\..\\nlp_pdfs\\2211.01910.pdf\n",
      "..\\..\\nlp_pdfs\\2212.03551.pdf\n",
      "..\\..\\nlp_pdfs\\2212.12633.pdf\n",
      "..\\..\\nlp_pdfs\\2302.02676.pdf\n",
      "..\\..\\nlp_pdfs\\2302.07842.pdf\n",
      "..\\..\\nlp_pdfs\\2302.09051.pdf\n",
      "..\\..\\nlp_pdfs\\2302.10329.pdf\n",
      "..\\..\\nlp_pdfs\\2302.13971.pdf\n",
      "..\\..\\nlp_pdfs\\2303.03846.pdf\n",
      "..\\..\\nlp_pdfs\\2303.07610.pdf\n",
      "..\\..\\nlp_pdfs\\2303.07992.pdf\n",
      "..\\..\\nlp_pdfs\\2303.08774.pdf\n",
      "..\\..\\nlp_pdfs\\2303.11504.pdf\n",
      "..\\..\\nlp_pdfs\\2303.11568.pdf\n",
      "..\\..\\nlp_pdfs\\2303.17491.pdf\n",
      "..\\..\\nlp_pdfs\\2303.18223.pdf\n",
      "..\\..\\nlp_pdfs\\2304.00612.pdf\n",
      "..\\..\\nlp_pdfs\\2304.01481.pdf\n",
      "..\\..\\nlp_pdfs\\2304.01665.pdf\n",
      "..\\..\\nlp_pdfs\\2304.09842.pdf\n",
      "..\\..\\nlp_pdfs\\2305.03047.pdf\n",
      "..\\..\\nlp_pdfs\\2306.00890.pdf\n",
      "..\\..\\nlp_pdfs\\Ago+03.pdf\n",
      "..\\..\\nlp_pdfs\\dong21a.pdf\n",
      "..\\..\\nlp_pdfs\\journal.pdig.0000198.pdf\n",
      "..\\..\\nlp_pdfs\\NIPS-2017-attention-is-all-you-need-Paper.pdf\n",
      "Failed:\n",
      "..\\..\\nlp_pdfs\\2303.08774.pdf\n",
      "..\\..\\nlp_pdfs\\2304.01665.pdf\n"
     ]
    }
   ],
   "source": [
    "failed_pdfs = []\n",
    "upsert_pdfs = Path(root).glob(\"**/*.pdf\")\n",
    "for this_pdf in upsert_pdfs:\n",
    "    try:\n",
    "        print(this_pdf)\n",
    "        loader = PyPDFLoader(str(this_pdf))\n",
    "        texts = loader.load()\n",
    "        docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=PINECONE_INDEX)\n",
    "    except:\n",
    "        failed_pdfs.append(str(this_pdf))\n",
    "        \n",
    "print('Failed:')\n",
    "for bad_pdf in failed_pdfs:\n",
    "    print(bad_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20583b8b-9fab-49e8-bf88-8bbee6fa5d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 963}},\n",
       " 'total_vector_count': 963}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a181f47-3789-4a67-80f3-f18c6ed89ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9c060c4-75ef-4ad4-a92b-3c344023c587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='language models can generate chains of thought if demonstrations of chain-of-thought reasoning are\\nprovided in the exemplars for few-shot prompting.\\nFigure 1 shows an example of a model producing a chain of thought to solve a math word problem\\nthat it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution\\nand can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it\\nmimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations\\ntypically come after the ﬁnal answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al.,\\n2022, inter alia )).\\nChain-of-thought prompting has several attractive properties as an approach for facilitating reasoning\\nin language models.\\n1.First, chain of thought, in principle, allows models to decompose multi-step problems into\\nintermediate steps, which means that additional computation can be allocated to problems\\nthat require more reasoning steps.\\n2.Second, a chain of thought provides an interpretable window into the behavior of the model,\\nsuggesting how it might have arrived at a particular answer and providing opportunities\\nto debug where the reasoning path went wrong (although fully characterizing a model’s\\ncomputations that support an answer remains an open question).\\n3.Third, chain-of-thought reasoning can be used for tasks such as math word problems,\\ncommonsense reasoning, and symbolic manipulation, and is potentially applicable (at least\\nin principle) to any task that humans can solve via language.\\n4.Finally, chain-of-thought reasoning can be readily elicited in sufﬁciently large off-the-shelf\\nlanguage models simply by including examples of chain of thought sequences into the\\nexemplars of few-shot prompting.\\nIn empirical experiments, we will observe the utility of chain-of-thought prompting for arithmetic\\nreasoning (Section 3), commonsense reasoning (Section 4), and symbolic reasoning (Section 5).\\n3 Arithmetic Reasoning\\nWe begin by considering math word problems of the form in Figure 1, which measure the arithmetic\\nreasoning ability of language models. Though simple for humans, arithmetic reasoning is a task where\\nlanguage models often struggle (Hendrycks et al., 2021; Patel et al., 2021, inter alia ). Strikingly, chain-\\nof-thought prompting when used with the 540B parameter language model performs comparably with\\ntask-speciﬁc ﬁnetuned models on several tasks, even achieving new state of the art on the challenging\\nGSM8K benchmark (Cobbe et al., 2021).\\n3.1 Experimental Setup\\nWe explore chain-of-thought prompting for various language models on multiple benchmarks.\\nBenchmarks. We consider the following ﬁve math word problem benchmarks: (1)theGSM8K\\nbenchmark of math word problems (Cobbe et al., 2021), (2)theSV AMP dataset of math word\\nproblems with varying structures (Patel et al., 2021), (3)theASDiv dataset of diverse math word\\nproblems (Miao et al., 2020), (4)theAQuA dataset of algebraic word problems, and (5)theMA WPS\\nbenchmark (Koncel-Kedziorski et al., 2016). Example problems are given in Appendix Table 12.\\nStandard prompting. For the baseline, we consider standard few-shot prompting, popularized by\\nBrown et al. (2020), in which a language model is given in-context exemplars of input–output pairs\\nbefore outputting a prediction for a test-time example. Exemplars are formatted as questions and\\nanswers. The model gives the answer directly, as shown in Figure 1 (left).\\nChain-of-thought prompting. Our proposed approach is to augment each exemplar in few-shot\\nprompting with a chain of thought for an associated answer, as illustrated in Figure 1 (right). As most\\nof the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars\\nwith chains of thought for prompting—Figure 1 (right) shows one chain of thought exemplar, and the\\nfull set of exemplars is given in Appendix Table 20. (These particular exemplars did not undergo\\nprompt engineering; robustness is studied in Section 3.4 and Appendix A.2.) To investigate whether\\nchain-of-thought prompting in this form can successfully elicit successful reasoning across a range of\\n3', metadata={}),\n",
       " Document(page_content='Chain-of-Thought Prompting Elicits Reasoning\\nin Large Language Models\\nJason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma\\nBrian Ichter Fei Xia Ed H. Chi Quoc V . Le Denny Zhou\\nGoogle Research, Brain Team\\n{jasonwei,dennyzhou}@google.com\\nAbstract\\nWe explore how generating a chain of thought —a series of intermediate reasoning\\nsteps—signiﬁcantly improves the ability of large language models to perform\\ncomplex reasoning. In particular, we show how such reasoning abilities emerge\\nnaturally in sufﬁciently large language models via a simple method called chain-of-\\nthought prompting , where a few chain of thought demonstrations are provided as\\nexemplars in prompting.\\nExperiments on three large language models show that chain-of-thought prompting\\nimproves performance on a range of arithmetic, commonsense, and symbolic\\nreasoning tasks. The empirical gains can be striking. For instance, prompting a\\nPaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art\\naccuracy on the GSM8K benchmark of math word problems, surpassing even\\nﬁnetuned GPT-3 with a veriﬁer.\\nA: The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer is 9.Chain-of-Thought PromptingQ: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: The answer is 11. Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?A: The answer is 27.Standard Prompting\\nQ: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?Model Input\\nModel OutputModel OutputModel Input\\nFigure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic,\\ncommonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted.\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2201.11903v6  [cs.CL]  10 Jan 2023', metadata={}),\n",
       " Document(page_content='C Extended Related Work\\nChain-of-thought prompting is a general approach that is inspired by several prior directions: prompt-\\ning, natural language explanations, program synthesis/execution, numeric and logical reasoning, and\\nintermediate language steps.\\nC.1 Prompting\\nThe recent success of large-scale language models has led to growing interest in improving their\\ncapability to perform tasks via prompting (Brown et al. (2020), and see Liu et al. (2021) for a\\nsurvey). This paper falls in the category of general prompting approaches, whereby input prompts are\\noptimized to allow a single large language model to better perform a variety of tasks (Li and Liang,\\n2021; Lester et al., 2021; Reif et al., 2022, inter alia ).\\nOne recent line of work aims to improve the ability of language models to perform a task by providing\\ninstructions that describe the task (Raffel et al., 2020; Wei et al., 2022a; Ouyang et al., 2022; Sanh\\net al., 2022; Wang et al., 2022b). This line of work is related because it also augments input–output\\npairs with meta-data. But whereas an instruction augments the input to a task (instructions are typically\\nprepended to the inputs), chain-of-thought prompting augments the outputs of language models.\\nAnother related direction is sequentially combining the outputs of language models; human–computer\\ninteraction (HCI) work (Wu et al., 2022a,b) has shown that combining sequential generations of\\nlanguage models improves task outcomes in a 20-person user study.\\nC.2 Natural language explanations\\nAnother closely related direction uses natural language explanations (NLEs), often with the goal of\\nimproving model interpretability (Zhou et al., 2020; Wiegreffe and Marasovi ´c, 2021, inter alia ). That\\nline of work typically focuses on natural language inference (Camburu et al., 2018; Yordanov et al.,\\n2021; Bostrom et al., 2021), and produces explanations either simultaneously to or after the ﬁnal\\nprediction (Narang et al., 2020; Majumder et al., 2021; Wiegreffe et al., 2021, 2022). By contrast,\\nthe chain of thought processing considered in this paper occurs before the ﬁnal answer. And while\\nNLE aims mostly to improve neural network interpretability (Rajagopal et al., 2021), the goal of\\nchain-of-thought prompting is to allow models to decompose multi-hop reasoning tasks into multiple\\nsteps—interpretability is just a side effect. Marasovi ´c et al. (2022) show that prompt-based ﬁnetuning\\nwith NLE improves NLI and classiﬁcation performance, though they largely focus on evaluating\\nexplanation plausibility. In comparison, our work focuses on a range of arithmetic, commonsense,\\nand symbolic tasks that require multi-hop reasoning.\\nC.3 Program synthesis and execution\\nUsing intermediate reasoning steps has a long history in program synthesis and execution (Zaremba\\nand Sutskever, 2014, inter alia ). Recent work along in this direction has included a number of\\narchitectural innovations (Cai et al., 2017; Dong et al., 2019; Yan et al., 2020), as well as the use of\\nlarge language models (Chen et al., 2021; Austin et al., 2021). The program execution work closest to\\nours is perhaps Nye et al. (2021), which show that large language models can perform up to 10-digit\\naddition, evaluate polynomials, and execute python programs. Whereas generating a program and\\nthen executing it can be viewed as a type of reasoning, our work generalizes such domain-speciﬁc\\nprimitives to natural language, which is open-domain and relevant to any text-to-text NLP task in\\nprinciple.\\nC.4 Numeric and logical reasoning\\nNumeric and logical reasoning has been a long-studied task in machine learning and natural language\\nprocessing (Lev et al., 2004, inter alia ). Recent work has also aimed to inject numeric reasoning\\nabilities in language models in various ways, such as augmenting BERT with a predeﬁned set of\\nexecutable operations (Andor et al., 2019), including a graph neural network (Ran et al., 2019), and\\nusing specialized training procedures (Pi˛ ekos et al., 2021). Another line of work aims to enable\\nlanguage models to perform logical or formal reasoning, often by verablizing the rules in natural\\nlanguage formal rules using language (Clark et al., 2020; Saeed et al., 2021; Liang et al., 2021).\\n24', metadata={}),\n",
       " Document(page_content='experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought\\nreasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,\\nchain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In\\nall experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language\\nmodel. No language models were ﬁnetuned in the process of writing this paper.\\nThe emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme\\n(Wei et al., 2022b). For many reasoning tasks where standard prompting has a ﬂat scaling curve, chain-\\nof-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting\\nappears to expand the set of tasks that large language models can perform successfully—in other\\nwords, our work underscores that standard prompting only provides a lower bound on the capabilities\\nof large language models. This observation likely raises more questions than it answers—for instance,\\nhow much more can we expect reasoning ability to improve with a further increase in model scale?\\nWhat other prompting methods might expand the range of tasks that language models can solve?\\nAs for limitations, we ﬁrst qualify that although chain of thought emulates the thought processes of\\nhuman reasoners, this does not answer whether the neural network is actually “reasoning,” which\\nwe leave as an open question. Second, although the cost of manually augmenting exemplars with\\nchains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for\\nﬁnetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot\\ngeneralization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct\\nand incorrect answers; improving factual generations of language models is an open direction for\\nfuture work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022, inter alia ). Finally,\\nthe emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in\\nreal-world applications; further research could explore how to induce reasoning in smaller models.\\n7 Related Work\\nThis work is inspired by many research areas, which we detail in an extended related work section\\n(Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.\\nThe ﬁrst relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017)\\npioneer the idea of using natural language rationales to solve math word problems through a series\\nof intermediate steps. Their work is a remarkable contrast to the literature using formal languages\\nto reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe\\net al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to ﬁnetune a pretrained\\nlanguage model rather than training a model from scratch. In the domain of program synthesis,\\nNye et al. (2021) leverage language models to predict the ﬁnal outputs of Python programs via\\nﬁrst line-to-line predicting the intermediate computational results, and show that their step-by-step\\nprediction method performs better than directly predicting the ﬁnal outputs.\\nNaturally, this paper also relates closely to the large body of recent work on prompting. Since the\\npopularization of few-shot prompting as given by Brown et al. (2020), several general approaches\\nhave improved the prompting ability of models, such as automatically learning prompts (Lester et al.,\\n2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang\\net al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g.,\\ninstructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the\\noutputs of language models with a chain of thought.\\n8 Conclusions\\nWe have explored chain-of-thought prompting as a simple and broadly applicable method for enhanc-\\ning reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense\\nreasoning, we ﬁnd that chain-of-thought reasoning is an emergent property of model scale that allows\\nsufﬁciently large language models to perform reasoning tasks that otherwise have ﬂat scaling curves.\\nBroadening the range of reasoning tasks that language models can perform will hopefully inspire\\nfurther work on language-based approaches to reasoning.\\n9', metadata={})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what is chain of thought prompting?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0757d1c-e929-4d77-b95b-4a9ef7b506a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Large Language Models are Zero-Shot Reasoners\\nTakeshi Kojima\\nThe University of Tokyo\\nt.kojima@weblab.t.u-tokyo.ac.jpShixiang Shane Gu\\nGoogle Research, Brain Team\\nMachel Reid\\nGoogle Research\\x03Yutaka Matsuo\\nThe University of TokyoYusuke Iwasawa\\nThe University of Tokyo\\nAbstract\\nPretrained large language models (LLMs) are widely used in many sub-ﬁelds of\\nnatural language processing (NLP) and generally known as excellent few-shot\\nlearners with task-speciﬁc exemplars. Notably, chain of thought (CoT) prompting,\\na recent technique for eliciting complex multi-step reasoning through step-by-\\nstep answer examples, achieved the state-of-the-art performances in arithmetics\\nand symbolic reasoning, difﬁcult system-2 tasks that do not follow the standard\\nscaling laws for LLMs. While these successes are often attributed to LLMs’\\nability for few-shot learning, we show that LLMs are decent zero-shot reasoners\\nby simply adding “Let’s think step by step” before each answer. Experimental\\nresults demonstrate that our Zero-shot-CoT, using the same single prompt template,\\nsigniﬁcantly outperforms zero-shot LLM performances on diverse benchmark\\nreasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SV AMP),\\nsymbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date\\nUnderstanding, Tracking Shufﬂed Objects), without any hand-crafted few-shot\\nexamples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and\\nGSM8K from 10.4% to 40.7% with large-scale InstructGPT model (text-davinci-\\n002), as well as similar magnitudes of improvements with another off-the-shelf\\nlarge model, 540B parameter PaLM. The versatility of this single prompt across\\nvery diverse reasoning tasks hints at untapped and understudied fundamental\\nzero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive\\ncapabilities may be extracted by simple prompting. We hope our work not only\\nserves as the minimal strongest zero-shot baseline for the challenging reasoning\\nbenchmarks, but also highlights the importance of carefully exploring and analyzing\\nthe enormous zero-shot knowledge hidden inside LLMs before crafting ﬁnetuning\\ndatasets or few-shot exemplars.\\n1 Introduction\\nScaling up the size of language models has been key ingredients of recent revolutions in natural\\nlanguage processing (NLP) [Vaswani et al., 2017, Devlin et al., 2019, Raffel et al., 2020, Brown et al.,\\n2020, Thoppilan et al., 2022, Rae et al., 2021, Chowdhery et al., 2022]. The success of large language\\nmodels (LLMs) is often attributed to (in-context) few-shot or zero-shot learning. It can solve various\\ntasks by simply conditioning the models on a few examples (few-shot) or instructions describing the\\ntask (zero-shot). The method of conditioning the language model is called “prompting” [Liu et al.,\\n2021b], and designing prompts either manually [Schick and Schütze, 2021, Reynolds and McDonell,\\n2021] or automatically [Gao et al., 2021, Shin et al., 2020] has become a hot topic in NLP.\\n\\x03Work done while at The University of Tokyo.\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2205.11916v4  [cs.CL]  29 Jan 2023', metadata={}),\n",
       " Document(page_content='Sanh et al. [2022], Ouyang et al. [2022] show that such zero-shot abilities of LLMs can be increased\\nby explicitly ﬁne-tuning models to follow instructions. Although these work focus on the zero-shot\\nperformances of LLMs, we focus on many system-2 tasks beyond system-1 tasks, considered a grand\\nchallenge for LLMs given ﬂat scaling curves. In addition, Zero-shot-CoT is orthogonal to instruction\\ntuning; it increases zero-shot performance for Instruct GPT3, vanilla GPT3, and PaLM (See Figure 3).\\nFrom Narrow (task-speciﬁc) to Broad (multi-task) Prompting Most prompts are task-speciﬁc.\\nWhile few-shot prompts are naturally so due to task-speciﬁc in-context samples [Brown et al., 2020,\\nWei et al., 2022], majority of zero-shot prompts have also focused on per-task engineering (of\\ntemplates) [Liu et al., 2021b, Reynolds and McDonell, 2021]. Borrowing terminologies from Chollet\\n[2019] which builds on hierarchical models of intelligence [McGrew, 2005, Johnson and Bouchard Jr,\\n2005], these prompts are arguably eliciting “narrow generalization” or task-speciﬁc skills from LLMs.\\nOn the other hand, our method is a multi-task prompt and elicits “broad generalization” or broad\\ncognitive abilities in LLMs, such as logical reasoning or system-2 itself. We hope our work can serve\\nas a reference for accelerating not just logical reasoning research with LLMs, but also discovery of\\nother broad cognitive capabilities within LLMs.\\nTraining Dataset Details A limitation of the work is the lack of public information on the details\\nof training datasets used for LLMs, e.g. 001 vs 002 for GPT models, original GPT3 vs Instruct-\\nGPT [Ouyang et al., 2022], and data for PaLM models [Chowdhery et al., 2022]. However, big\\nperformance increases from Zero-shot to Zero-shot-CoT in all recent large models (InstructGPT\\n001 or 002, Original GPT3, and PaLM) and consistent improvements in both arithmetic and non-\\narithmetic tasks suggest that the models are unlikely simply memorizing, but instead capturing a\\ntask-agnostic multi-step reasoning capability for generic problem solving. While most results are\\nbased on InstructGPT since it is the best performing open-access LLM, key results are reproduced\\non PaLM, and dataset details in InstructGPT (Appendix A, B, and F in Ouyang et al. [2022]) also\\nconﬁrm that it is not specially engineered for multi-step reasoning.\\nLimitation and Social Impact Our work is based on prompting methods for large language models.\\nLLMs have been trained on large corpora from various sources on the web (also see “Training Dataset\\nDetails”), and have shown to capture and amplify biases found in the training data. Prompting is a\\nmethod that looks to take advantage of the patterns captured by language models conducive to various\\ntasks, and therefore it has the same shortcomings. This being said, our approach is a more direct way\\nto probe complex reasoning inside pre-trained LLMs, removing the confounding factor of in-context\\nlearning in prior few-shot approaches, and can lead to more unbiased study of biases in LLMs.\\n6 Conclusion\\nWe have proposed Zero-shot-CoT, a single zero-shot prompt that elicits chain of thought from large\\nlanguage models across a variety of reasoning tasks, in contrast to the few-shot (in-context) approach\\nin previous work that requires hand-crafting few-shot examples per task. Our simple method not only\\nis the minimalist and strongest zero-shot baseline for difﬁcult multi-step system-2 reasoning tasks\\nthat long evaded the scaling laws of LLMs, but also encourages the community to further discover\\nsimilar multi-task prompts that elicit broad cognitive abilities instead of narrow task-speciﬁc skills.\\nAcknowledgements\\nThis work has been supported by the Mohammed bin Salman Center for Future Science and Technol-\\nogy for Saudi-Japan Vision 2030 at The University of Tokyo (MbSC2030). Computational resource\\nof AI Bridging Cloud Infrastructure (ABCI) provided by National Institute of Advanced Industrial\\nScience and Technology (AIST) was used for experiments other than PaLM. We also thank Jason\\nWei and Denny Zhou for discussions and support on running PaLM experiments, and Sharan Narang\\nand Aakanksha Chowdhery for generic support on PaLM infrastructures.\\nReferences\\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\\nFinn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian\\n10', metadata={}),\n",
       " Document(page_content='020406080\\n0.3B 1.3B 6.7B 175BZero-shot Zero-shot-CoT (a) MultiArith on Original GPT-3\\n020406080\\nS M L XLZero-shot Zero-shot-CoT (b) MultiArith on Instruct GPT-3\\n01020304050\\n8B 62B 540BZero-shot Zero-shot-CoT (c) GMS8K on PaLM\\nFigure 3: Model scale study with various types of models. S: text-ada-001, M: text-babbage-001, L:\\ntext-curie-001, XL: text-davinci-002. See Appendix A.3 and E for the detail.\\nTable 3: Examples generated by Zero-Shot-CoT on CommonsenseQA for Error Analysis.\\nExample 1 (Reasoning is correct.) Example 2 (Model cannot choose only one.)\\nQ:Where is a well used toy car likely to be found?\\nAnswer Choices: (A) child’s room (B) boy’s bed-\\nroom (C) own home (D) toy store (E) houseQ:What would be necessary for getting in shape?\\nAnswer Choices: (A) good health (B) exercise (C)\\nmuscle tone (D) sweat (E) feel better\\nA: Let’s think step by step. A toy car is likely\\nto be found in a child’s room. A child’s room is\\nlikely to be found in a house. Therefore, a toy\\ncar is likely to be found in a house.\\nTherefore, among A through E, the answer is E.\\n7(GT : A, Zero-shot: A)A: Let’s think step by step. In order to get in shape,\\nyou need to be able to do three things:\\n1. Exercise regularly\\n2. Eat a healthy diet\\n3. Get enough sleep\\nIf you can do all three of those things, then you will\\nbe well on your way to getting in shape! Therefore,\\namong A through E, the answer is B, C, and D.\\n(GT : B, Zero-shot: B)\\nComparison with other baselines Table 2 compares the performances on two arithmetic reasoning\\nbenchmarks (MultiArith and GSM8K) across Zero-shot-CoT and baselines. The large gap between\\nstandard prompting (1st block) and chain of thought prompting (2nd block) suggests that these\\ntasks are difﬁcult without eliciting multi-step reasoning. Major improvements are conﬁrmed on\\nboth Instruct GPT-3 (text-davinci-002) and PaLM (540B) models (4th block). While Zero-shot-CoT\\nnaturally underperforms Few-shot-CoT, it substantially outperforms standard Few-shot prompting\\nwith even 8 examples per task. For GSM8K, Zero-shot-CoT with Instruct GPT-3 (text-davinci-002)\\nalso outperforms ﬁnetuned GPT-3 and standard few-shot prompting with large models (PaLM, 540B),\\nreported in Wei et al. [2022] (3rd and 4th block). See App. D for more experiment results with PaLM.\\nDoes model size matter for zero-shot reasoning? Figure 3 compares performance of various\\nlanguage models on MultiArith / GSM8K. Without chain of thought reasoning, the performance\\ndoes not increase or increases slowly as the model scale is increased, i.e., the curve is mostly ﬂat. In\\ncontrast, the performance drastically increases with chain of thought reasoning, as the model size\\ngets bigger, for Original/Instruct GPT-3 and PaLM. When the model size is smaller, chain of thought\\nreasoning is not effective. This result aligns with the few-shot experiment results in Wei et al. [2022].\\nAppendix E shows extensive experiment results using wider variety of language models, including\\nGPT-2, GPT-Neo, GPT-J, T0, and OPT. We also manually investigated the quality of generated chain\\nof thought, and large-scale models clearly demonstrate better reasoning (See Appendix B for the\\nsampled outputs for each model).\\nError Analysis To better understand the behavior of Zero-shot-CoT, we manually investigated\\nrandomly selected examples generated by Instruct-GPT3 with Zero-shot-CoT prompting. See Ap-\\npendix C for examples, where some of the observations include: (1) In commonsense reasoning\\n(CommonsenseQA), Zero-shot-CoT often produces ﬂexible and reasonable chain of thought even\\nwhen the ﬁnal prediction is not correct. Zero-shot-CoT often output multiple answer choices when\\nthe model ﬁnd it is difﬁcult to narrow it down to one (see Table 3 for examples). (2) In arithmetic\\n7', metadata={}),\n",
       " Document(page_content='model (text-davinci-002). We also evaluate Zero-shot-CoT with another off-the-shelf large model,\\n540B parameter PaLM, showing similar magnitudes of improvements on MultiArith and GSM8K.\\nImportantly, with our single ﬁxed prompt, zero-shot LLMs have a signiﬁcantly better scaling curve\\ncomparable to that of the few-shot CoT baseline. We also show that besides Few-shot-CoT requiring\\nhuman engineering of multi-step reasoning prompts, their performance deteriorates if prompt example\\nquestion types and task question type are unmatched, suggesting high sensitivity to per-task prompt\\ndesigns. In contrast, the versatility of this single prompt across diverse reasoning tasks hints at\\nuntapped and understudied zero-shot fundamental capabilities of LLMs, such as higher-level broad\\ncognitive capabilities like generic logical reasoning [Chollet, 2019]. While the vibrant ﬁeld of LLMs\\nstarted out from the premise of excellent few-shot learners [Brown et al., 2020], we hope our work\\nencourages more research into uncovering high-level andmulti-task zero-shot capabilities hidden\\ninside those models.\\n2 Background\\nWe brieﬂy review the two core preliminary concepts that form the basis of this work: the advent of\\nlarge language models (LLMs) and prompting, and chain of thought (CoT) prompting for multi-step\\nreasoning.\\nLarge language models and prompting A language model (LM), is a model that looks to estimate\\nthe probability distribution over text. Recently, scaling improvements through larger model sizes\\n(from a few million [Merity et al., 2016] to hundreds of millions [Devlin et al., 2019] to hundreds of\\nbillions [Brown et al., 2020] parameters) and larger data (e.g. webtext corpora [Gao et al., 2020])\\nhave enabled pre-trained large language models (LLMs) to be incredibly adept at many downstream\\nNLP tasks. Besides the classic “pre-train and ﬁne-tune” paradigm [Liu et al., 2021b], models scaled\\nto 100B+ parameters exhibit properties conducive to few-shot learning [Brown et al., 2020], by way\\nof in context learning, where one can use a text or template known as a prompt to strongly guide the\\ngeneration to output answers for desired tasks, thus beginning an era of “pre-train and prompt” [Liu\\net al., 2021a]. In work, we call such prompts with explicit conditioning on few task examples as\\nfew-shot prompts, and other template-only prompts as zero-shot prompts.\\nChain of thought prompting Multi-step arithmetic and logical reasoning benchmarks have par-\\nticularly challenged the scaling laws of large language models [Rae et al., 2021]. Chain of thought\\n(CoT) prompting [Wei et al., 2022], an instance of few-shot prompting, proposed a simple solution\\nby modifying the answers in few-shot examples to step-by-step answers, and achieved signiﬁcant\\nboosts in performance across these difﬁcult benchmarks, especially when combined with very large\\nlanguage models like PaLM [Chowdhery et al., 2022]. The top row of Figure 1 shows standard\\nfew-shot prompting against (few-shot) CoT prompting. Notably, few-shot learning was taken as a\\ngiven for tackling such difﬁcult tasks, and the zero-shot baseline performances were not even reported\\nin the original work [Wei et al., 2022]. To differentiate it from our method, we call Wei et al. [2022]\\nasFew-shot-CoT in this work.\\n3 Zero-shot Chain of Thought\\nWe propose Zero-shot-CoT, a zero-shot template-based prompting for chain of thought reasoning.\\nIt differs from the original chain of thought prompting [Wei et al., 2022] as it does not require\\nstep-by-step few-shot examples, and it differs from most of the prior template prompting [Liu et al.,\\n2021b] as it is inherently task-agnostic and elicits multi-hop reasoning across a wide range of tasks\\nwith a single template. The core idea of our method is simple, as described in Figure 1: add Let’s\\nthink step by step , or a a similar text (see Table 4), to extract step-by-step reasoning.\\n3.1 Two-stage prompting\\nWhile Zero-shot-CoT is conceptually simple, it uses prompting twice to extract both reasoning and\\nanswer, as explained in Figure 2. In contrast, the zero-shot baseline (see the bottom-left in Figure 1)\\nalready uses prompting in the form of “The answer is”, to extract the answers in correct formats.\\nFew-shot prompting, standard or CoT, avoids needing such answer-extraction prompting by explicitly\\ndesigning the few-shot example answers to end in such formats (see the top-right and top-left\\n3', metadata={})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what is zero shot learning?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "672b2070-19aa-44d7-aa92-5125543a120e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Published in Transactions on Machine Learning Research (08/2022)\\nEmergent Abilities of Large Language Models\\nJason Wei1jasonwei@google.com\\nYi Tay1yitay@google.com\\nRishi Bommasani2nlprishi@stanford.edu\\nColin Raﬀel3craﬀel@gmail.com\\nBarret Zoph1barretzoph@google.com\\nSebastian Borgeaud4sborgeaud@deepmind.com\\nDani Yogatama4dyogatama@deepmind.com\\nMaarten Bosma1bosma@google.com\\nDenny Zhou1dennyzhou@google.com\\nDonald Metzler1metzler@google.com\\nEd H. Chi1edchi@google.com\\nTatsunori Hashimoto2thashim@stanford.edu\\nOriol Vinyals4vinyals@deepmind.com\\nPercy Liang2pliang@stanford.edu\\nJeﬀ Dean1jeﬀ@google.com\\nWilliam Fedus1liamfedus@google.com\\n1Google Research2Stanford University3UNC Chapel Hill4DeepMind\\nReviewed on OpenReview: https://openreview.net/forum?id=yzkSU5zdwD\\nAbstract\\nScaling up language models has been shown to predictably improve performance and sample\\neﬃciency on a wide range of downstream tasks. This paper instead discusses an unpredictable\\nphenomenon that we refer to as emergent abilities of large language models. We consider an\\nability to be emergent if it is not present in smaller models but is present in larger models.\\nThus, emergent abilities cannot be predicted simply by extrapolating the performance of\\nsmaller models. The existence of such emergence raises the question of whether additional\\nscaling could potentially further expand the range of capabilities of language models.\\n1 Introduction\\nLanguage models have revolutionized natural language processing (NLP) in recent years. It is now well-known\\nthat increasing the scale of language models (e.g., training compute, model parameters, etc.) can lead to\\nbetter performance and sample eﬃciency on a range of downstream NLP tasks (Devlin et al., 2019; Brown\\net al., 2020, inter alia ). In many cases, the eﬀect of scale on performance can often be methodologically\\npredicted via scaling laws—for example, scaling curves for cross-entropy loss have been shown to empirically\\nspan more than seven orders of magnitude (Kaplan et al., 2020; Hoﬀmann et al., 2022). On the other hand,\\nperformance for certain downstream tasks counterintuitively does not appear to continuously improve as a\\nfunction of scale, and such tasks cannot be predicted ahead of time (Ganguli et al., 2022).\\nIn this paper, we will discuss the unpredictable phenomena of emergent abilities of large language models.\\nEmergence as an idea has been long discussed in domains such as physics, biology, and computer science\\n(Anderson, 1972; Hwang et al., 2012; Forrest, 1990; Corradini & O’Connor, 2010; Harper & Lewis, 2012, inter\\n1arXiv:2206.07682v2  [cs.CL]  26 Oct 2022', metadata={}),\n",
       " Document(page_content='Published in Transactions on Machine Learning Research (08/2022)\\nalia). We will consider the following general deﬁnition of emergence, adapted from Steinhardt (2022) and\\nrooted in a 1972 essay called “More Is Diﬀerent” by Nobel prize-winning physicist Philip Anderson (Anderson,\\n1972):\\nEmergence is when quantitative changes in a system result in qualitative changes in behavior.\\nHere we will explore emergence with respect to model scale, as measured by training compute and number of\\nmodel parameters. Speciﬁcally, we deﬁne emergent abilities of large language models as abilities that are\\nnot present in smaller-scale models but are present in large-scale models; thus they cannot be predicted by\\nsimply extrapolating the performance improvements on smaller-scale models (§2).1We survey emergent\\nabilities as observed in a range of prior work, categorizing them in settings such as few-shot prompting (§3)\\nand augmented prompting strategies (§4). Emergence motivates future research on why such abilities are\\nacquired and whether more scaling will lead to further emergent abilities, which we highlight as important\\nquestions for the ﬁeld (§5).\\n2 Emergent Abilities Deﬁnition\\nAs a broad concept, emergence is often used informally and can be reasonably interpreted in many diﬀerent\\nways. In this paper, we will consider a focused deﬁnition of emergent abilities of large language models:\\nAn ability is emergent if it is not present in smaller models but is present in larger models.\\nEmergent abilities would not have been directly predicted by extrapolating a scaling law (i.e. consistent\\nperformance improvements) from small-scale models. When visualized via a scaling curve ( x-axis: model\\nscale, y-axis: performance), emergent abilities show a clear pattern—performance is near-random until a\\ncertain critical threshold of scale is reached, after which performance increases to substantially above random.\\nThis qualitative change is also known as a phase transition —a dramatic change in overall behavior that would\\nnot have been foreseen by examining smaller-scale systems (Huberman & Hogg, 1987).\\nToday’s language models have been scaled primarily along three factors: amount of computation, number\\nof model parameters, and training dataset size (Kaplan et al., 2020; Hoﬀmann et al., 2022). In this paper,\\nwe will analyze scaling curves by plotting the performance of diﬀerent models where training compute for\\neach model is measured in FLOPs on the x-axis (Hoﬀmann et al., 2022). Because language models trained\\nwith more compute tend to also have more parameters, we additionally show plots with number of model\\nparameters as the x-axis in Appendix D (see Figure 11 and Figure 12, as well as Figure 4 and Figure 10).\\nUsing training FLOPs or model parameters as the x-axis produces curves with similar shapes due to the fact\\nthat most dense Transformer language model families have scaled training compute roughly proportionally\\nwith model parameters (Kaplan et al., 2020).\\nTraining dataset size is also an important factor, but we do not plot capabilities against it because many\\nlanguage model families use a ﬁxed number of training examples for all model sizes (Brown et al., 2020; Rae\\net al., 2021; Chowdhery et al., 2022). Although we focus on training computation and model size here, there\\nis not a single proxy that adequately captures all aspects of scale. For example, Chinchilla (Hoﬀmann et al.,\\n2022) has one-fourth as many parameters as Gopher (Rae et al., 2021) but uses similar training compute; and\\nsparse mixture-of-expert models have more parameters per training/inference compute than dense models\\n(Fedus et al., 2021; Du et al., 2021). Overall, it may be wise to view emergence as a function of many\\ncorrelated variables. For example, later in Figure 4 we will also plot emergence as a function of WikiText103\\nperplexity (Merity et al., 2016), which happens to closely correlate with training computation for Gopher/\\nChinchilla (though this correlation may not hold in the long-run).\\nNote that the scale at which an ability is ﬁrst observed to emerge depends on a number of factors and is\\nnot an immutable property of the ability. For instance, emergence may occur with less training compute\\n1This survey focuses on pre-trained Transformer language models. Emergent abilities in NLP more broadly, however, could\\ngo back to Miller et al. (2004), Liang (2005), or earlier.\\n2', metadata={}),\n",
       " Document(page_content='4\\nEmergent Abilities of LLMs . In the literature [31], emergent\\nabilities of LLMs are formally deﬁned as “the abilities that\\nare not present in small models but arise in large models”,\\nwhich is one of the most prominent features that distin-\\nguish LLMs from previous PLMs. It further introduces a\\nnotable characteristic when emergent abilities occur [31]:\\nperformance rises signiﬁcantly above random when the\\nscale reaches a certain level. By analogy, such an emergent\\npattern has close connections with the phenomenon of phase\\ntransition in physics [31, 58]. In principle, emergent abilities\\ncan be deﬁned in relation to some complex tasks [31, 59],\\nwhile we are more concerned with general abilities that\\ncan be applied to solve a variety of tasks. Here, we brieﬂy\\nintroduce three typical emergent abilities for LLMs and\\nrepresentative models that possess such an ability7.\\n\\x0fIn-context learning. The in-context learning (ICL) abil-\\nity is formally introduced by GPT-3 [55]: assuming that\\nthe language model has been provided with a natural\\nlanguage instruction and/or several task demonstrations,\\nit can generate the expected output for the test instances\\nby completing the word sequence of input text, without\\nrequiring additional training or gradient update8. Among\\nthe GPT-series models, the 175B GPT-3 model exhibited\\na strong ICL ability in general, but not the GPT-1 and\\nGPT-2 models. While, such an ability also depends on the\\nspeciﬁc downstream task. For example, the ICL ability can\\nemerge on the arithmetic tasks ( e.g., the 3-digit addition and\\nsubtraction) for the 13B GPT-3, but 175B GPT-3 even cannot\\nwork well on the Persian QA task [31].\\n\\x0fInstruction following. By ﬁne-tuning with a mixture of\\nmulti-task datasets formatted via natural language descrip-\\ntions (called instruction tuning ), LLMs are shown to perform\\nwell on unseen tasks that are also described in the form\\nof instructions [28, 61, 62]. With instruction tuning, LLMs\\nare enabled to follow the task instructions for new tasks\\nwithout using explicit examples, thus having an improved\\ngeneralization ability. According to the experiments in [62],\\ninstruction-tuned LaMDA-PT [63] started to signiﬁcantly\\noutperform the untuned one on unseen tasks when the\\nmodel size reached 68B, but not for 8B or smaller model\\nsizes. A recent study [64] found that a model size of 62B is\\nat least required for PaLM to perform well on various tasks\\nin four evaluation benchmarks ( i.e.,MMLU, BBH, TyDiQA\\nand MGSM), though a much smaller size might sufﬁce for\\nsome speciﬁc tasks ( e.g., MMLU).\\n\\x0fStep-by-step reasoning. For small language models, it is\\nusually difﬁcult to solve complex tasks that involve multiple\\nreasoning steps, e.g., mathematical word problems. While,\\nwith the chain-of-thought (CoT) prompting strategy [33],\\nLLMs can solve such tasks by utilizing the prompting\\nmechanism that involves intermediate reasoning steps for\\nderiving the ﬁnal answer. This ability is speculated to be\\npotentially obtained by training on code [33, 47]. An empir-\\nical study [33] has shown that CoT prompting can bring\\n7. It is difﬁcult to accurately examine the critical size for emergent\\nabilities of LLMs ( i.e.,the minimum size to possess an ability), since it\\nmight vary for different models or tasks. Besides, existing studies often\\ntest emergent abilities on very limited model sizes for a speciﬁc LLM.\\nFor example, PaLM is often tested with three sizes of 8B, 62B and 540B.\\nIt is unclear about the model performance of the untested sizes.\\n8. In a recent study [60], it also shows that in-context learning implic-\\nitly performs meta-optimization through the attention mechanism.performance gains (on arithmetic reasoning benchmarks)\\nwhen applied to PaLM and LaMDA variants with a model\\nsize larger than 60B, while its advantage over the standard\\nprompting becomes more evident when the model size\\nexceeds 100B. Besides, the performance improvement with\\nCoT prompting seems to be also varied for different tasks,\\ne.g., GSM8K>MAWPS>SWAMP for PaLM [33].\\nKey Techniques for LLMs . It has been a long way that\\nLLMs evolve into the current state: general and capable\\nlearners. In the development process, a number of impor-\\ntant techniques are proposed, which largely improve the\\ncapacity of LLMs. Here, we brieﬂy list several important\\ntechniques that (potentially) lead to the success of LLMs, as\\nfollows.\\n\\x0fScaling . As discussed in previous parts, there exists\\nan evident scaling effect in Transformer language mod-\\nels: larger model/data sizes and more training compute\\ntypically lead to an improved model capacity [30, 34]. As\\ntwo representative models, GPT-3 and PaLM explored the\\nscaling limits by increasing the model size to 175B and\\n540B, respectively. Furthermore, since compute budget is\\nusually limited, scaling laws can be employed to conduct a\\nmore compute-efﬁcient allocation of the compute resources.\\nFor example, Chinchilla (with more training tokens) outper-\\nforms its counterpart model Gopher (with a larger model\\nsize) by increasing the data scale with the same compute\\nbudget [34]. While, it should be noted that data scaling\\nshould be with careful cleaning process, since the quality\\nof pre-training data plays a key role in the model capacity.\\n\\x0fTraining . Due to the huge model size, it is very chal-\\nlenging to successfully train a capable LLM. Distributed\\ntraining algorithms are needed to learn the network param-\\neters of LLMs, in which various parallel strategies are often\\njointly utilized. To support distributed training, several opti-\\nmization frameworks have been released to facilitate the im-\\nplementation and deployment of parallel algorithms, such\\nas DeepSpeed [65] and Megatron-LM [66–68]. Besides, opti-\\nmization tricks are also important for training stability and\\nmodel performance, e.g., restart to overcome training loss\\nspike [56] and mixed precision training [69]. More recently,\\nGPT-4 [46] proposes to develop special infrastructure and\\noptimization methods that reliably predict the performance\\nof large models with much smaller models.\\n\\x0fAbility eliciting . After being pre-trained on large-scale\\ncorpora, LLMs are endowed with potential abilities as\\ngeneral-purpose task solvers. While, these abilities might\\nnot be explicitly exhibited when LLMs perform some spe-\\nciﬁc tasks. As the technical approach, it is useful to de-\\nsign suitable task instructions or speciﬁc in-context learn-\\ning strategies to elicit such abilities. For instance, chain-\\nof-thought prompting has been shown to be useful to\\nsolve complex reasoning tasks by including intermediate\\nreasoning steps. Besides, we can further perform instruction\\ntuning on LLMs with task descriptions expressed in natural\\nlanguage, for improving the generalizability of LLMs on\\nunseen tasks. While, these techniques mainly correspond to\\nthe emergent abilities of LLMs, which may not show the\\nsame effect on small language models.\\n\\x0fAlignment tuning . Since LLMs are trained to capture\\nthe data characteristics of pre-training corpora (including', metadata={}),\n",
       " Document(page_content='Published in Transactions on Machine Learning Research (08/2022)\\nor fewer model parameters for models trained on higher-quality data. Conversely, emergent abilities also\\ncrucially depend on other factors such as not being limited by the amount of data, its quality, or the number\\nof parameters in the model. Today’s language models are likely not trained optimally (Hoﬀmann et al., 2022),\\nand our understanding of how to best train models will evolve over time. Our goal in this paper is not to\\ncharacterize or claim that a speciﬁc scale is required to observe emergent abilities, but rather, we aim to\\ndiscuss examples of emergent behavior in prior work.\\n3 Few-Shot Prompted Tasks\\nLanguage \\nmodelInput\\nOutput Review: This movie sucks.\\nSentiment: negative.\\nReview: I love this movie.\\nSentiment:\\npositive.\\nFigure 1: Example of an input and\\noutput for few-shot prompting.We ﬁrst discuss emergent abilities in the prompting paradigm, as pop-\\nularized by GPT-3 (Brown et al., 2020).2In prompting, a pre-trained\\nlanguage model is given a prompt (e.g. a natural language instruction)\\nof a task and completes the response without any further training\\nor gradient updates to its parameters. Brown et al. (2020) proposed\\nfew-shot prompting , which includes a few input-output examples in\\nthe model’s context (input) as a preamble before asking the model to\\nperform the task for an unseen inference-time example. An example prompt is shown in Figure 1.\\nThe ability to perform a task via few-shot prompting is emergent when a model has random performance\\nuntil a certain scale, after which performance increases to well-above random. Figure 2 shows eight such\\nemergent abilities spanning ﬁve language model families from various work.\\nBIG-Bench. Figure 2A–D depicts four emergent few-shot prompted tasks from BIG-Bench, a crowd-sourced\\nsuite of over 200 benchmarks for language model evaluation (BIG-Bench, 2022). Figure 2A shows an arithmetic\\nbenchmark that tests 3-digit addition and subtraction, as well as 2-digit multiplication. GPT-3 and LaMDA\\n(Thoppilan et al., 2022) have close-to-zero performance for several orders of magnitude of training compute,\\nbefore performance jumps to sharply above random at 2·1022training FLOPs (13B parameters) for GPT-3,\\nand 1023training FLOPs (68B parameters) for LaMDA. Similar emergent behavior also occurs at around the\\nsame model scale for other tasks, such as transliterating from the International Phonetic Alphabet (Figure 2B),\\nrecovering a word from its scrambled letters (Figure 2C), and Persian question-answering (Figure 2D). Even\\nmore emergent abilities from BIG-Bench are given in Appendix E.\\nTruthfulQA. Figure 2E shows few-shot prompted performance on the TruthfulQA benchmark, which\\nmeasures the ability to answer questions truthfully (Lin et al., 2021). This benchmark is adversarially curated\\nagainst GPT-3 models, which do not perform above random, even when scaled to the largest model size.\\nSmall Gopher models also do not perform above random until scaled up to the largest model of 5·1023\\ntraining FLOPs (280B parameters), for which performance jumps to more than 20% above random (Rae\\net al., 2021).\\nGrounded conceptual mappings. Figure 2F shows the task of grounded conceptual mappings, where\\nlanguage models must learn to map a conceptual domain, such as a cardinal direction, represented in a\\ntextual grid world (Patel & Pavlick, 2022). Again, performance only jumps to above random using the largest\\nGPT-3 model.\\nMulti-task language understanding. Figure 2G shows the Massive Multi-task Language Understanding\\n(MMLU) benchmark, which aggregates 57 tests covering a range of topics including math, history, law, and\\nmore (Hendrycks et al., 2021a). For GPT-3, Gopher, and Chinchilla, models of ∼1022training FLOPs ( ∼10B\\nparameters) or smaller do not perform better than guessing on average over all the topics, scaling up to 3–5\\n·1023training FLOPs (70B–280B parameters) enables performance to substantially surpass random. This\\nresult is striking because it could imply that the ability to solve knowledge-based questions spanning a large\\ncollection of topics might require scaling up past this threshold (for dense language models without retrieval\\nor access to external memory).\\n2Though GPT-3 popularized prompting, the task setup has existed since before GPT-3 (Trinh & Le, 2018; McCann et al.,\\n2018; Radford et al., 2019; Raﬀel et al., 2020).\\n3', metadata={})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what are emergent abilities?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
