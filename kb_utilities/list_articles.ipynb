{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9df66c-e3aa-42f1-9d0f-fee0ab442ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5825135e-5888-43ef-ab6a-6e9dc5ea29f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196e9b2d-d4e5-4a3f-affd-8a7dcefa2c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf97967c-193d-47fd-8bc4-9dd57dffcf93",
   "metadata": {},
   "source": [
    "### Upsert pdfs\n",
    "\n",
    "This notebook is a down and dirty way to upload a directory of pdfs to a pinecone database.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45ad90d-dabc-470e-9f6b-c7c24b3be2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\installed_software\\anaconda3\\envs\\langchain\\Lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "import openai\n",
    "import os\n",
    "\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader, PyPDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea2decf-d1b5-46a0-98ed-41a972a9770f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4accec3e-db14-442c-9591-7fbbb84fdcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is ready\n"
     ]
    }
   ],
   "source": [
    "with open('D:\\\\NLP_projects\\\\openai_api_key.txt', 'r') as file:\n",
    "    OPENAI_API_KEY = file.read().strip()\n",
    "\n",
    "\n",
    "with open('D:\\\\NLP_projects\\\\youtube_api_key.txt', 'r') as file:\n",
    "    YOUTUBE_API_KEY = file.read().strip()\n",
    "    \n",
    "with open('D:\\\\NLP_projects\\\\pinecone_api_key.txt', 'r') as file:\n",
    "    PINECONE_API_KEY = file.read().strip()\n",
    "    \n",
    "PINECONE_API_ENV = 'northamerica-northeast1-gcp'\n",
    "\n",
    "PINECONE_INDEX = 'denson-kb'\n",
    "    \n",
    "# Set OpenAI API Key    \n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\") is not None:\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    print (\"OPENAI_API_KEY is ready\")\n",
    "else:\n",
    "    print (\"OPENAI_API_KEY environment variable not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a9c82fd-f2e3-44f9-a99e-9e11a37b52dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_API_ENV)\n",
    "index = pinecone.Index(PINECONE_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b5038f7-591b-4a6b-9cac-8f949ad0d79d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 963}},\n",
       " 'total_vector_count': 963}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f55676f5-a9b9-4778-8a6e-0285f9a90823",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # If you want to delete your vectors in your index to start over, run the code below!\n",
    "# index = pinecone.Index(index_name)\n",
    "# index.delete(delete_all='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82dee421-0f1a-4967-8a79-2aaaf5ebb270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 963}},\n",
       " 'total_vector_count': 963}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e3d6d08-dbbe-4201-b63b-097edf878142",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e1ceddb-1c5b-4687-a6ca-a962c34d27ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = PyPDFDirectoryLoader(\"../../nlp_pdfs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203dee3b-13f0-4ca7-9560-60e84f047eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "553301e7-f1ee-41cb-9ba1-c729680aba44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base='', openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization='', allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=6, request_timeout=None, headers=None, tiktoken_model_name=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20583b8b-9fab-49e8-bf88-8bbee6fa5d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 963}},\n",
       " 'total_vector_count': 963}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a181f47-3789-4a67-80f3-f18c6ed89ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4b1b90a-a4d5-4271-8eae-36a5ac4132fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \" \"\n",
    "\n",
    "# embedded_query = embeddings.embed_query(query)\n",
    "embedded_query = [0] * 1536\n",
    "docs = index.query(embedded_query, top_k=1000,include_values=False,include_metadata=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cdb3dee-2a7c-4f8f-9228-c9268459aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_docs = docs.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcf53285-5f56-402b-a8c5-9d91fdfe02fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "963"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_docs['matches'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00282a39-4bc9-4711-b721-65c691d52505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5ba35d40-626f-423f-9823-ae822890947d',\n",
       " 'score': 0.0,\n",
       " 'values': [],\n",
       " 'metadata': {'page': 13.0,\n",
       "  'source': '..\\\\..\\\\nlp_pdfs\\\\2302.07842.pdf',\n",
       "  'text': 'LM needs to ground itself in the real-world by learning about aﬀordances i.e. what actions are possible in a\\ngiven state, and their eﬀect on the world.\\nControlling Virtual Agents. Recent works demonstrated the ability of LMs to control virt ual agents\\nin simulated 2D and 3D environments by outputting functions which can then be executed by computer\\nin the corresponding environment, be it a simulation or the r eal-world. For example, Li et al. (2022b ) ﬁne-\\ntune a pre-trained GPT2 (Radford et al. ,2019) on sequential decision-making problems by representing t he\\ngoals and observations as a sequence of embeddings and predi cting the next action. This framework enables\\nstrong combinatorial generalization across diﬀerent doma ins including a simulated household environment.\\nThis suggests that LMs can produce representations that are useful for modeling not only language but\\nalso sequential goals and plans, so that they can improve lea rning and generalization on tasks that go\\nbeyond language processing. Similarly, Huang et al. (2022a ) investigate whether it is possible to use the\\nworld knowledge captured by LMs to take speciﬁc actions in re sponse to high-level tasks written in natural\\nlanguage such as “make breakfast” . This work was the ﬁrst to d emonstrate that if the LM is large enough and\\ncorrectly prompted, it can break down high-level tasks into a series of simple commands without additional\\ntraining. However, the agent has access to a predetermined s et of actions, so not all natural language\\ncommands can be executed in the environment. To address this issue, the authors propose to map the\\ncommands suggested by the LM into feasible actions for the ag ent using the cosine similarity function. The\\napproach is evaluated in a virtual household environment an d displays an improvement in the ability to\\nexecute tasks compared to using the plans generated by the LM without the additional mapping. While\\nthese works have demonstrated the usefulness of LMs for cont rolling virtual robots, the following paragraph\\ncover works on physical robots. Zeng et al. (2022) combine a LM with a visual-language model (VLM) and\\na pre-trained language-conditioned policy for controllin g a simulated robotic arm. The LM is used as a\\nmulti-step planner to break down a high-level task into subg oals, while the VLM is used to describe the\\nobjects in the scene. Both are passed to the policy which then executes actions according to the speciﬁed\\ngoal and observed state of the world. Dasgupta et al. (2023) use 7B and 70B Chinchilla as planners for\\nan agent that acts and observes the result in a PycoLab enviro nment. Additionally, a reporter module\\nconverts actions and observations from pixel to text space. Finally, the agent in Carta et al. (2023) uses a\\nLM to generate action policies for text-based tasks. Intera ctively learning via online RL allows to ground the\\nLM internal representations to the environment, thus partl y departing from the knowledge about statistical\\nsurface structure of text that was acquired during pre-trai ning.\\nCommand Eﬀect\\nsearch <query> Send <query> to the Bing API and display a search results page\\nclicked on link <link ID> Follow the link with the given ID to a new page\\nfind in page: <text> Find the next occurrence of <text> and scroll to it\\nquote: <text> If <text> is found in the current page, add it as a reference\\nscrolled down <1, 2, 3> Scroll down a number of times\\nscrolled up <1, 2, 3> Scroll up a number of times\\nTop Scroll to the top of the page\\nback Go to the previous page\\nend: answer End browsing and move to answering phase\\nend: <nonsense, controversial> End browsing and skip answering phase\\nTable 3: The actions WebGPT can perform, taken from Nakano et al. (2021).\\nControlling Physical Robots. Liang et al. (2022) use a LM to write robot policy code given natural lan-\\nguage commands by prompting the model with a few demonstrati ons. By combining classic logic structures\\nand referencing external libraries, e.g., for arithmetic o perations, LMs can create policies that exhibit spatial-\\ngeometric reasoning, generalize to new instructions, and p rovide precise values for ambiguous descriptions.\\nThe eﬀectiveness of the approach is demonstrated on multipl e real robot platforms. LMs encode common\\nsense knowledge about the world which can be useful in gettin g robots to follow complex high-level instruc-\\ntions expressed in natural language. However, they lack con textual grounding which makes it diﬃcult to use\\n14'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_docs['matches'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3462240-6539-41c3-8a8d-4425d1f1662a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches\n",
      "namespace\n"
     ]
    }
   ],
   "source": [
    "for the_key in dict_docs.keys():\n",
    "    print(the_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0757d1c-e929-4d77-b95b-4a9ef7b506a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docsearch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is zero shot learning?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m docs \u001b[38;5;241m=\u001b[39m docsearch\u001b[38;5;241m.\u001b[39msimilarity_search(query)\n\u001b[0;32m      3\u001b[0m docs\n",
      "\u001b[1;31mNameError\u001b[0m: name 'docsearch' is not defined"
     ]
    }
   ],
   "source": [
    "query = \"what is zero shot learning?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b2070-19aa-44d7-aa92-5125543a120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what are emergent abilities?\"\n",
    "docs = index.similarity_search(query)\n",
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
